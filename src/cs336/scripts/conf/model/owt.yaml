vocab_size: 32000
context_length: 512
d_model: 768
d_ff: 3072
num_heads: 12
num_layers: 6
theta: 10000.0
ffn_type: "relu_squared"
qk_norm: false
